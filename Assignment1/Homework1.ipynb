{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID2222 - Homework 1\n",
    "\n",
    "You are to implement the stages of finding textually similar documents based on Jaccard similarity using the shingling, minhashing, and locality-sensitive hashing (LSH) techniques and corresponding algorithms. The implementation can be done using any big data processing framework, such as Apache Spark, Apache Flink, or no framework, e.g., in Java, Python, etc. To test and evaluate your implementation, write a program that uses your implementation to find similar documents in a corpus of 5-10 or more documents such as web pages or emails.\n",
    "\n",
    "The stages should be implemented as a collection of classes, modules, functions or procedures depending the framework and the language of your choice. Below, we give a description of sample classes that implement different stages of finding textually similar documents. You do not have to develop the exact same classes and data types as described below. Feel free to use data structures that suit you best.\n",
    "\n",
    "1. A class Shingling that constructs k–shingles of a given length k (e.g., 10) from a given document, computes a hash value for each unique shingle, and represents the document in the form of an ordered set of its hashed k-shingles.\n",
    "2. A class CompareSets that computes the Jaccard similarity of two sets of integers – two sets of hashed shingles.\n",
    "3. A class MinHashing that builds a minHash signature (in the form of a vector or a set) of a given length n from a given set of integers (a set of hashed shingles).\n",
    "4. A class CompareSignatures that estimates similarity of two integer vectors – minhash signatures – as a fraction of components, in which they agree.\n",
    "5. (Optional task for extra 2 bonus) A class LSH that implements the LSH technique: given a collection of minhash signatures (integer vectors) and a similarity threshold t, the LSH class (using banding and hashing) finds all candidate pairs of signatures that agree on at least fraction t of their components.\n",
    "\n",
    "To test and evaluate scalability (the execution time versus the size of input dataset) of your implementation, write a program that uses your classes to find similar documents in a corpus of 5-10 documents. Choose a similarity threshold s (e.g., 0,8) that states that two documents are similar if the Jaccard similarity of their shingle sets is at least s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability\n",
    "To test and evaluate scalability (the execution time versus the size of input dataset) of your implementation, write a program that uses your classes to find similar documents in a corpus of 5-10 documents. Choose a similarity threshold s (e.g., 0,8) that states that two documents are similar if the Jaccard similarity of their shingle sets is at least s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_and_prepare_data(path):\n",
    "    with open(\"train-v2.0.json\") as f:\n",
    "        d = f.read()\n",
    "        data = json.loads(d)\n",
    "\n",
    "    characters = {}\n",
    "    for d in data['data']:\n",
    "        info = d['paragraphs'][0]['context']\n",
    "\n",
    "        characters[d['title']] = info\n",
    "\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "def load_business():\n",
    "    path = \"bbc-full-text-document-classification/bbc/business\"\n",
    "    file_list = os.listdir(path)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(path, file)) as f:\n",
    "            data[file.split(\".\")[0]] = f.read().replace('\\n', ' ').lower()\n",
    "    return data\n",
    "\n",
    "def load_conrad():\n",
    "    path = \"conradbooks\"\n",
    "    file_list = os.listdir(path)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(path, file)) as f:\n",
    "            data[file.split(\".\")[0]] = f.read().replace('\\n', ' ').lower().replace(string.punctuation, \"\")\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_conrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_and_prepare_data(\"train-v2.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "import time\n",
    "\n",
    "class Shingling:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.docs_shingles = {}\n",
    "        self.doc_names = []\n",
    "        \n",
    "    def _clean(self, doc):\n",
    "        \"\"\"\n",
    "        Some rules for cleaning the text:\n",
    "        https://www.cs.utah.edu/~jeffp/teaching/cs5955/L4-Jaccard+Shingle.pdf\n",
    "        \"\"\"\n",
    "        doc = doc.lower()\n",
    "        doc = doc.replace(\" \", \"_\")\n",
    "        return doc\n",
    "    \n",
    "    def _tokenize(self, doc):\n",
    "        \"\"\"\n",
    "        Construct the shingles based on k-characters\n",
    "        \"\"\"\n",
    "        sh = set()\n",
    "        if len(doc) >= self.k:\n",
    "            for idx, token in enumerate(doc):\n",
    "                if idx + self.k <= len(doc):\n",
    "                    sh.add(self._hash(doc[idx:idx + self.k]))\n",
    "\n",
    "        return sh\n",
    "\n",
    "    def _hash(self, shingle):\n",
    "        \"\"\"\n",
    "        Compute hash values for the shingle\n",
    "        \"\"\"\n",
    "        return binascii.crc32(shingle.encode(\"utf-8\")) & 0xffffffff\n",
    "    \n",
    "    def generate_shingles(self, doc):\n",
    "        doc = self._clean(doc)\n",
    "        shingles = self._tokenize(doc)\n",
    "        \n",
    "        return shingles  \n",
    "    \n",
    "    def generate_shingles_for_docs(self, docs):\n",
    "        \"\"\"\n",
    "        Takes in docs in the form of a dict of {\"docID\": \"doc string\"}\n",
    "        \"\"\"\n",
    "        print(\"Shingling {} articles...\".format(len(docs)))\n",
    "\n",
    "        t0 = time.time()\n",
    "        for k, v in docs.items():\n",
    "            self.doc_names.append(k)\n",
    "            d = self._clean(v)\n",
    "            d = self._tokenize(d)\n",
    "    \n",
    "            self.docs_shingles[k] = d\n",
    "    \n",
    "        print ('\\nShingling took %.2f sec.' % (time.time() - t0))\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_sets(s1, s2):\n",
    "        \"\"\"\n",
    "        Compute Jaccard Similarity\n",
    "        n(intersection) / n(union)\n",
    "        \"\"\"\n",
    "        # add in some checks\n",
    "        if(s1 == set() or s2 == set()):\n",
    "            print(\"Warning: at least one of the two set is empty\\n\")\n",
    "            return 0\n",
    "        else:\n",
    "            jacc_sim = (len(s1.intersection(s2)) / float(len(s1.union(s2))))\n",
    "            return jacc_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shing = Shingling(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling 10 articles...\n",
      "\n",
      "Shingling took 3.32 sec.\n"
     ]
    }
   ],
   "source": [
    "shing.generate_shingles_for_docs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1739170626190168"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shing.compare_sets(shing.docs_shingles[shing.doc_names[1]],\n",
    "                   shing.docs_shingles[shing.doc_names[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_jaccard_sim(docs_shingles, doc_names):\n",
    "    print(\"Calculating the Jaccard Similarity for all documents\")\n",
    "    dataset_size = len(docs_shingles)\n",
    "\n",
    "    jaccSimMatrix = np.zeros(dataset_size * dataset_size).reshape(dataset_size,dataset_size)\n",
    "    t0 = time.time()\n",
    "\n",
    "    docKeys = list(docs_shingles.keys())\n",
    "    for j in range(0, len(doc_names)):    \n",
    "        s1 = docs_shingles[doc_names[j]]\n",
    "        for k in range(j, len(doc_names)):\n",
    "            s2 = docs_shingles[doc_names[k]]\n",
    "            if(s1 == set() or s2 == set()):\n",
    "                print(\"Warning: at least one of the two set is empty\\n\")\n",
    "                jacc_sim = 0\n",
    "            else:\n",
    "                jacc_sim = (len(s1.intersection(s2)) / float(len(s1.union(s2))))\n",
    "            jaccSimMatrix[j, k] = jacc_sim\n",
    "            jaccSimMatrix[k, j] = jacc_sim\n",
    "\n",
    "    print ('\\nJaccard Similarity for ' + str(len(doc_names)) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    "    print ('\\nJaccard Similarity Matrix\\n ' + str(jaccSimMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the Jaccard Similarity for all documents\n",
      "\n",
      "Jaccard Similarity for 10 docs took 2.38 sec.\n",
      "\n",
      "Jaccard Similarity Matrix\n",
      " [[1.         0.16113816 0.21036779 0.22820762 0.21872387 0.20052262\n",
      "  0.16820954 0.22315445 0.2077853  0.22019119]\n",
      " [0.16113816 1.         0.17391706 0.12242617 0.17244878 0.201923\n",
      "  0.22293174 0.13569971 0.18379233 0.1543728 ]\n",
      " [0.21036779 0.17391706 1.         0.21260154 0.22143697 0.2152056\n",
      "  0.18705407 0.22008889 0.21683682 0.22518434]\n",
      " [0.22820762 0.12242617 0.21260154 1.         0.21161193 0.18434147\n",
      "  0.13867327 0.26457318 0.19503701 0.22545309]\n",
      " [0.21872387 0.17244878 0.22143697 0.21161193 1.         0.22098586\n",
      "  0.18859731 0.21821277 0.21574418 0.22023137]\n",
      " [0.20052262 0.201923   0.2152056  0.18434147 0.22098586 1.\n",
      "  0.21503294 0.19661474 0.21942763 0.20335983]\n",
      " [0.16820954 0.22293174 0.18705407 0.13867327 0.18859731 0.21503294\n",
      "  1.         0.15455349 0.19693328 0.1690633 ]\n",
      " [0.22315445 0.13569971 0.22008889 0.26457318 0.21821277 0.19661474\n",
      "  0.15455349 1.         0.19903824 0.23480533]\n",
      " [0.2077853  0.18379233 0.21683682 0.19503701 0.21574418 0.21942763\n",
      "  0.19693328 0.19903824 1.         0.20323798]\n",
      " [0.22019119 0.1543728  0.22518434 0.22545309 0.22023137 0.20335983\n",
      "  0.1690633  0.23480533 0.20323798 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "jaccSimMatrix  = generate_jaccard_sim(shing.docs_shingles, shing.doc_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "class MinHashing:\n",
    "    \n",
    "    def __init__(self, n, max_shingle_ID = 2**32-1):\n",
    "        self.n = n # number of hashes\n",
    "        self.max_shingle_ID = max_shingle_ID # the max number\n",
    "        self.next_prime = 4294967311 # the next prime number after max shingle ID\n",
    "        self.coeffs_A = self.generate_coeffs()\n",
    "        self.coeffs_B = self.generate_coeffs()\n",
    "        self.docs_minhash_signatures = {}\n",
    "    \n",
    "    def generate_coeffs(self):\n",
    "        \"\"\"\n",
    "        Create a list of 'n' unique random values.\n",
    "        \"\"\"\n",
    "        coeffs_list = []\n",
    "        \n",
    "        for _ in range(self.n):\n",
    "            # TODO: check if it a good idea to have 0 for coeff A\n",
    "            rand_idx = random.randint(0, self.max_shingle_ID)\n",
    "\n",
    "            # Ensure that each random number is unique.\n",
    "            while rand_idx in coeffs_list:\n",
    "                rand_idx = random.randint(0, self.max_shingle_ID)\n",
    "\n",
    "            coeffs_list.append(rand_idx)\n",
    "\n",
    "        return coeffs_list\n",
    "\n",
    "    def _minHash_function(self, pos, x):\n",
    "        \"\"\"\n",
    "        Return a hash in the form of (ax+b) % prime\n",
    "        \"\"\"\n",
    "        return (self.coeffs_A[pos] * x + self.coeffs_B[pos]) % self.next_prime\n",
    "        \n",
    "    def generate_signature(self, shingle_set):\n",
    "        \"\"\"\n",
    "        Given a shingle set of IDs, generate the hashes and compute the minimum hash\n",
    "        \"\"\"\n",
    "        signature = []\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            signature.append(min(map(lambda x: self._minHash_function(i,x), shingle_set)))\n",
    "\n",
    "        return signature\n",
    "    \n",
    "    def generate_doc_signatures(self, shingles):\n",
    "        print(\"Generating MinHash signatures for documents..\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        for k, v in shingles.items():\n",
    "            self.docs_minhash_signatures[k] = self.generate_signature(v)\n",
    "       \n",
    "        print ('\\n Generating Signatures for ' + str(len(shingles)) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_signatures(s1, s2):\n",
    "        if not len(s1) == len(s2):\n",
    "            print(\"Unequal length of Signature\")\n",
    "            \n",
    "        equality = 0\n",
    "        signature_len = len(s1)\n",
    "        for x, y in zip(s1, s2):\n",
    "            if(x == y):\n",
    "                equality += 1 \n",
    "        return equality / float(signature_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHashing(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minhash.generate_signature(shing.docs_shingles[shing.doc_names[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.145"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minhash.compare_signatures(minhash.generate_signature(shing.docs_shingles[shing.doc_names[1]]),\n",
    "                   minhash.generate_signature(shing.docs_shingles[shing.doc_names[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating MinHash signatures for documents..\n",
      "\n",
      " Generating Signatures for 10 docs took 170.74 sec.\n"
     ]
    }
   ],
   "source": [
    "minhash.generate_doc_signatures(shing.docs_shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH\n",
    "Partition into Bands\n",
    "- Divide matrix M into b bands of r rows.\n",
    "- For each band, hash its portion of each column to a hash table with k buckets.\n",
    "- Make k as large as possible.\n",
    "- Candidate column pairs are those that hash to the same bucket for a number of bands with regards to the threshold set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class LSH:\n",
    "    \n",
    "    def __init__(self, band_size, row_size, threshold):\n",
    "        self.band_size = band_size\n",
    "        self.threshold = threshold\n",
    "        self.row_size = row_size\n",
    "        self.docs_lsh = {}\n",
    "        self.candidate_pairs = defaultdict(set)\n",
    "\n",
    "    def get_lsh(self, signature):\n",
    "        lsh = []\n",
    "        for i in range(self.band_size):\n",
    "            lsh.append(hash(tuple(signature[i*self.row_size:(i*self.row_size+self.row_size)])) % 4294967311)\n",
    "        return lsh\n",
    "\n",
    "    def get_lsh_for_docs(self, signatures):\n",
    "        for k, v in signatures.items():\n",
    "            self.docs_lsh[k] = self.get_lsh(v)\n",
    "        \n",
    "    def generate_candidate_pairs(self, t=0.0002):\n",
    "        \"\"\"\n",
    "        t: the fraction of components that pair of signatures agrees on\n",
    "        \"\"\"\n",
    "        all_docs = list(self.docs_lsh.values())\n",
    "        all_names = list(self.docs_lsh.keys())\n",
    "        \n",
    "        # Minimum number of bands that should has overlap\n",
    "        # hash according to the threshold set\n",
    "        threshold = t * self.band_size\n",
    "\n",
    "        # Stores the intermediate number of band overlaps\n",
    "        pairs = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "        for idx, s1 in enumerate(all_docs):\n",
    "            s1_name = all_names[idx]\n",
    "            \n",
    "            # Sliding count to perform comparison\n",
    "            for curr_iter, s2 in enumerate(all_docs[idx + 1:]):\n",
    "                s2_name = all_names[curr_iter + idx + 1]\n",
    "                \n",
    "                for x, y in zip(s1, s2):\n",
    "                    if(x == y):\n",
    "                        if not pairs[s1_name][s2_name]:\n",
    "                            pairs[s1_name][s2_name] = 1\n",
    "                        else:\n",
    "                            pairs[s1_name][s2_name] += 1\n",
    "\n",
    "                # Store pairs that is above the threshold as candidate pairs\n",
    "                if pairs[s1_name][s2_name] > threshold:\n",
    "                    self.candidate_pairs[s1_name].add(s2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lshh = LSH(50, 4, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3330187200,\n",
       " 2127868941,\n",
       " 519083102,\n",
       " 745080478,\n",
       " 3135366119,\n",
       " 2006144478,\n",
       " 50045948,\n",
       " 1332400039,\n",
       " 3980626399,\n",
       " 3801580741,\n",
       " 2497146791,\n",
       " 262966903,\n",
       " 1824525263,\n",
       " 1255471848,\n",
       " 3142506904,\n",
       " 3963954758,\n",
       " 27889379,\n",
       " 2144500702,\n",
       " 7864650,\n",
       " 1656721948,\n",
       " 1176055604,\n",
       " 3371242827,\n",
       " 311765484,\n",
       " 703792230,\n",
       " 1542726733,\n",
       " 1268895722,\n",
       " 1925046707,\n",
       " 996801477,\n",
       " 3631345277,\n",
       " 2767722317,\n",
       " 1170095390,\n",
       " 3564658967,\n",
       " 1729763392,\n",
       " 3002260798,\n",
       " 2684752910,\n",
       " 2806297386,\n",
       " 3015818423,\n",
       " 3010009412,\n",
       " 3644051409,\n",
       " 1800828627,\n",
       " 2017540586,\n",
       " 4197045970,\n",
       " 2533592844,\n",
       " 409855283,\n",
       " 2853379971,\n",
       " 1987280662,\n",
       " 876834819,\n",
       " 3333429166,\n",
       " 4092622166,\n",
       " 3740663004]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lshh.get_lsh(minhash.docs_minhash_signatures[shing.doc_names[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lshh.get_lsh_for_docs(minhash.docs_minhash_signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3330187200,\n",
       " 2127868941,\n",
       " 519083102,\n",
       " 745080478,\n",
       " 3135366119,\n",
       " 2006144478,\n",
       " 50045948,\n",
       " 1332400039,\n",
       " 3980626399,\n",
       " 3801580741,\n",
       " 2497146791,\n",
       " 262966903,\n",
       " 1824525263,\n",
       " 1255471848,\n",
       " 3142506904,\n",
       " 3963954758,\n",
       " 27889379,\n",
       " 2144500702,\n",
       " 7864650,\n",
       " 1656721948,\n",
       " 1176055604,\n",
       " 3371242827,\n",
       " 311765484,\n",
       " 703792230,\n",
       " 1542726733,\n",
       " 1268895722,\n",
       " 1925046707,\n",
       " 996801477,\n",
       " 3631345277,\n",
       " 2767722317,\n",
       " 1170095390,\n",
       " 3564658967,\n",
       " 1729763392,\n",
       " 3002260798,\n",
       " 2684752910,\n",
       " 2806297386,\n",
       " 3015818423,\n",
       " 3010009412,\n",
       " 3644051409,\n",
       " 1800828627,\n",
       " 2017540586,\n",
       " 4197045970,\n",
       " 2533592844,\n",
       " 409855283,\n",
       " 2853379971,\n",
       " 1987280662,\n",
       " 876834819,\n",
       " 3333429166,\n",
       " 4092622166,\n",
       " 3740663004]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lshh.docs_lsh[shing.doc_names[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lshh.generate_candidate_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {\"Almayer'sFolly\": {'ChanceATaleInTwoParts'},\n",
       "             'AmyFoster': {'The Secret Sharer'},\n",
       "             'Falk': {'TheArrowofGold'}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lshh.candidate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"This article:\\n\\n\", dataset['229'])\n",
    "#print(\"\\n\\nis similar to:\\n \", dataset['209'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
