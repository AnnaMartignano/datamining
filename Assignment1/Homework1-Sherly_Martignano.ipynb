{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID2222 - Homework 1\n",
    "\n",
    "**Authors: Sherly Sherly and Anna Martignano**\n",
    "\n",
    "You are to implement the stages of finding textually similar documents based on Jaccard similarity using the shingling, minhashing, and locality-sensitive hashing (LSH) techniques and corresponding algorithms. The implementation can be done using any big data processing framework, such as Apache Spark, Apache Flink, or no framework, e.g., in Java, Python, etc. To test and evaluate your implementation, write a program that uses your implementation to find similar documents in a corpus of 5-10 or more documents such as web pages or emails.\n",
    "\n",
    "The stages should be implemented as a collection of classes, modules, functions or procedures depending the framework and the language of your choice. Below, we give a description of sample classes that implement different stages of finding textually similar documents. You do not have to develop the exact same classes and data types as described below. Feel free to use data structures that suit you best.\n",
    "\n",
    "1. A class Shingling that constructs k–shingles of a given length k (e.g., 10) from a given document, computes a hash value for each unique shingle, and represents the document in the form of an ordered set of its hashed k-shingles.\n",
    "2. A class CompareSets that computes the Jaccard similarity of two sets of integers – two sets of hashed shingles.\n",
    "3. A class MinHashing that builds a minHash signature (in the form of a vector or a set) of a given length n from a given set of integers (a set of hashed shingles).\n",
    "4. A class CompareSignatures that estimates similarity of two integer vectors – minhash signatures – as a fraction of components, in which they agree.\n",
    "5. (Optional task for extra 2 bonus) A class LSH that implements the LSH technique: given a collection of minhash signatures (integer vectors) and a similarity threshold t, the LSH class (using banding and hashing) finds all candidate pairs of signatures that agree on at least fraction t of their components.\n",
    "\n",
    "To test and evaluate scalability (the execution time versus the size of input dataset) of your implementation, write a program that uses your classes to find similar documents in a corpus of 5-10 documents. Choose a similarity threshold s (e.g., 0,8) that states that two documents are similar if the Jaccard similarity of their shingle sets is at least s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cell\n",
    "import os\n",
    "import string\n",
    "import binascii\n",
    "import time\n",
    "import random \n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conrad():\n",
    "    path = \"conradbooks\"\n",
    "    file_list = os.listdir(path)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for file in file_list:\n",
    "        with open(os.path.join(path, file), 'rb') as f:\n",
    "            data[file.split(\".\")[0]] = f.read().decode('utf-8', errors='replace')\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_conrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shingling:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.docs_shingles = {}\n",
    "        self.doc_names = []\n",
    "        \n",
    "    def _clean(self, doc):\n",
    "        \"\"\"\n",
    "        Some rules for cleaning the text:\n",
    "        https://www.cs.utah.edu/~jeffp/teaching/cs5955/L4-Jaccard+Shingle.pdf\n",
    "        \"\"\"\n",
    "        doc = doc.lower().replace('\\n', ' ')\n",
    "        doc = re.sub('[^A-Za-z\\d\\s]', '', doc)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.replace(\" \", \"_\")\n",
    "        return doc\n",
    "    \n",
    "    def _tokenize(self, doc):\n",
    "        \"\"\"\n",
    "        Construct the shingles based on k-characters\n",
    "        \"\"\"\n",
    "        sh = set()\n",
    "        if len(doc) >= self.k:\n",
    "            for idx, token in enumerate(doc):\n",
    "                if idx + self.k <= len(doc):\n",
    "                    sh.add(self._hash(doc[idx:idx + self.k]))\n",
    "\n",
    "        return sh\n",
    "\n",
    "    def _hash(self, shingle):\n",
    "        \"\"\"\n",
    "        Compute hash values for the shingle\n",
    "        \"\"\"\n",
    "        return binascii.crc32(shingle.encode(\"utf-8\")) & 0xffffffff\n",
    "    \n",
    "    def generate_shingles(self, doc):\n",
    "        doc = self._clean(doc)\n",
    "        shingles = self._tokenize(doc)\n",
    "        \n",
    "        return shingles  \n",
    "    \n",
    "    def generate_shingles_for_docs(self, docs):\n",
    "        \"\"\"\n",
    "        Takes in docs in the form of a dict of {\"docID\": \"doc string\"}\n",
    "        \"\"\"\n",
    "        print(\"Shingling {} articles...\".format(len(docs)))\n",
    "\n",
    "        t0 = time.time()\n",
    "        for k, v in docs.items():\n",
    "            self.doc_names.append(k)\n",
    "            d = self._clean(v)\n",
    "            d = self._tokenize(d)\n",
    "    \n",
    "            self.docs_shingles[k] = d\n",
    "    \n",
    "        print ('\\nShingling took %.2f sec.' % (time.time() - t0))\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_sets(s1, s2):\n",
    "        \"\"\"\n",
    "        Compute Jaccard Similarity\n",
    "        n(intersection) / n(union)\n",
    "        \"\"\"\n",
    "        # add in some checks\n",
    "        if(s1 == set() or s2 == set()):\n",
    "            print(\"Warning: at least one of the two set is empty\\n\")\n",
    "            return 0\n",
    "        else:\n",
    "            jacc_sim = (len(s1.intersection(s2)) / float(len(s1.union(s2))))\n",
    "            return jacc_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shing = Shingling(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling 10 articles...\n",
      "\n",
      "Shingling took 15.10 sec.\n"
     ]
    }
   ],
   "source": [
    "shing.generate_shingles_for_docs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23643820346230315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shing.compare_sets(shing.docs_shingles[shing.doc_names[1]],\n",
    "                   shing.docs_shingles[shing.doc_names[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinHashing:\n",
    "    \n",
    "    def __init__(self, n, max_shingle_ID = 2**32-1):\n",
    "        self.n = n # number of hashes\n",
    "        self.max_shingle_ID = max_shingle_ID # the max number\n",
    "        self.next_prime = 4294967311 # the next prime number after max shingle ID\n",
    "        self.coeffs_A = self.generate_coeffs()\n",
    "        self.coeffs_B = self.generate_coeffs()\n",
    "        self.docs_minhash_signatures = {}\n",
    "    \n",
    "    def generate_coeffs(self):\n",
    "        \"\"\"\n",
    "        Create a list of 'n' unique random values.\n",
    "        \"\"\"\n",
    "        coeffs_list = []\n",
    "        \n",
    "        for _ in range(self.n):\n",
    "            # TODO: check if it a good idea to have 0 for coeff A\n",
    "            rand_idx = random.randint(0, self.max_shingle_ID)\n",
    "\n",
    "            # Ensure that each random number is unique.\n",
    "            while rand_idx in coeffs_list:\n",
    "                rand_idx = random.randint(0, self.max_shingle_ID)\n",
    "\n",
    "            coeffs_list.append(rand_idx)\n",
    "\n",
    "        return coeffs_list\n",
    "\n",
    "    def _minHash_function(self, pos, x):\n",
    "        \"\"\"\n",
    "        Return a hash in the form of (ax+b) % prime\n",
    "        \"\"\"\n",
    "        return (self.coeffs_A[pos] * x + self.coeffs_B[pos]) % self.next_prime\n",
    "        \n",
    "    def generate_signature(self, shingle_set):\n",
    "        \"\"\"\n",
    "        Given a shingle set of IDs, generate the hashes and compute the minimum hash\n",
    "        \"\"\"\n",
    "        signature = []\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            signature.append(min(map(lambda x: self._minHash_function(i,x), shingle_set)))\n",
    "\n",
    "        return signature\n",
    "    \n",
    "    def generate_doc_signatures(self, shingles):\n",
    "        print(\"Generating MinHash signatures for documents..\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        for k, v in shingles.items():\n",
    "            self.docs_minhash_signatures[k] = self.generate_signature(v)\n",
    "       \n",
    "        print ('\\n Generating Signatures for ' + str(len(shingles)) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_signatures(s1, s2):\n",
    "        if not len(s1) == len(s2):\n",
    "            print(\"Unequal length of Signature\")\n",
    "            \n",
    "        equality = 0\n",
    "        signature_len = len(s1)\n",
    "        for x, y in zip(s1, s2):\n",
    "            if(x == y):\n",
    "                equality += 1 \n",
    "        return equality / float(signature_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "minhash = MinHashing(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minhash.compare_signatures(minhash.generate_signature(shing.docs_shingles[shing.doc_names[1]]),\n",
    "                   minhash.generate_signature(shing.docs_shingles[shing.doc_names[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating MinHash signatures for documents..\n",
      "\n",
      " Generating Signatures for 10 docs took 364.79 sec.\n"
     ]
    }
   ],
   "source": [
    "minhash.generate_doc_signatures(shing.docs_shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH\n",
    "Partition into Bands\n",
    "- Divide matrix M into b bands of r rows.\n",
    "- For each band, hash its portion of each column to a hash table with k buckets.\n",
    "- Make k as large as possible.\n",
    "- Candidate column pairs are those that hash to the same bucket for a number of bands with regards to the threshold set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    \n",
    "    def __init__(self, band_size, row_size, threshold):\n",
    "        self.band_size = band_size\n",
    "        self.threshold = threshold\n",
    "        self.row_size = row_size\n",
    "        self.docs_lsh = {}\n",
    "        self.candidate_pairs = defaultdict(set)\n",
    "\n",
    "    def get_lsh(self, signature):\n",
    "        lsh = []\n",
    "        for i in range(self.band_size):\n",
    "            lsh.append(hash(tuple(signature[i*self.row_size:(i*self.row_size+self.row_size)])) % 4294967311)\n",
    "        return lsh\n",
    "\n",
    "    def get_lsh_for_docs(self, signatures):\n",
    "        for k, v in signatures.items():\n",
    "            self.docs_lsh[k] = self.get_lsh(v)\n",
    "        \n",
    "    def generate_candidate_pairs(self):\n",
    "        \"\"\"\n",
    "        t: the fraction of components that pair of signatures agrees on\n",
    "        \"\"\"\n",
    "        print(\"Generating MinHash signatures for documents..\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        all_docs = list(self.docs_lsh.values())\n",
    "        all_names = list(self.docs_lsh.keys())\n",
    "        \n",
    "        # Minimum number of bands that should has overlap\n",
    "        # hash according to the threshold set\n",
    "        threshold = self.threshold * self.band_size\n",
    "\n",
    "        # Stores the intermediate number of band overlaps\n",
    "        pairs = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "        for idx, s1 in enumerate(all_docs):\n",
    "            s1_name = all_names[idx]\n",
    "            \n",
    "            # Sliding count to perform comparison\n",
    "            for curr_iter, s2 in enumerate(all_docs[idx + 1:]):\n",
    "                s2_name = all_names[curr_iter + idx + 1]\n",
    "                \n",
    "                for x, y in zip(s1, s2):\n",
    "                    if(x == y):\n",
    "                        if not pairs[s1_name][s2_name]:\n",
    "                            pairs[s1_name][s2_name] = 1\n",
    "                        else:\n",
    "                            pairs[s1_name][s2_name] += 1\n",
    "                        \n",
    "                        if not pairs[s2_name][s1_name]:\n",
    "                            pairs[s2_name][s1_name] = 1\n",
    "                        else:\n",
    "                            pairs[s2_name][s1_name] += 1\n",
    "\n",
    "                # Store pairs that is above the threshold as candidate pairs\n",
    "                if pairs[s1_name][s2_name] > threshold:\n",
    "                    self.candidate_pairs[s1_name].add(s2_name)\n",
    "                    self.candidate_pairs[s2_name].add(s1_name)\n",
    "\n",
    "                    \n",
    "        print ('\\n Generating Signatures for ' + str(len(all_docs)) + ' docs took %.2f sec.' % (time.time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lshh = LSH(100, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lshh.get_lsh_for_docs(minhash.docs_minhash_signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating MinHash signatures for documents..\n",
      "\n",
      " Generating Signatures for 10 docs took 0.00 sec.\n"
     ]
    }
   ],
   "source": [
    "lshh.generate_candidate_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'APersonalRecord': {'TheArrowofGold'},\n",
       "             'ChanceATaleInTwoParts': {'TheArrowofGold'},\n",
       "             'TheArrowofGold': {'APersonalRecord', 'ChanceATaleInTwoParts'}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lshh.candidate_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability\n",
    "To test and evaluate scalability (the execution time versus the size of input dataset) of your implementation, write a program that uses your classes to find similar documents in a corpus of 5-10 documents. Choose a similarity threshold s (e.g., 0,8) that states that two documents are similar if the Jaccard similarity of their shingle sets is at least s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_sim_matrix(vectors, doc_names):\n",
    "    print(\"Calculating the Similarity for all documents\")\n",
    "    dataset_size = len(vectors)\n",
    "\n",
    "    simMatrix = np.zeros(dataset_size * dataset_size).reshape(dataset_size,dataset_size)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for j in range(0, len(doc_names)):    \n",
    "        s1 = vectors[doc_names[j]]\n",
    "        for k in range(j, len(doc_names)):\n",
    "            s2 = vectors[doc_names[k]]\n",
    "            if(s1 == set() or s2 == set()):\n",
    "                print(\"Warning: at least one of the two set is empty\\n\")\n",
    "                sim = 0\n",
    "            elif j == k:\n",
    "                sim = 0\n",
    "            else:\n",
    "                sim = (len(set(s1).intersection(set(s2))) / float(len(set(s1).union(set(s2)))))\n",
    "            simMatrix[j, k] = sim\n",
    "            simMatrix[k, j] = sim\n",
    "\n",
    "    print('\\nSimilarity for ' + str(len(doc_names)) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    "    np.set_printoptions(precision=3)\n",
    "    print('\\nSimilarity Matrix\\n ' + str(simMatrix))\n",
    "\n",
    "    return simMatrix\n",
    "    \n",
    "def retrieve_documents(sim_matrix, threshold=0.2, doc_names=None):\n",
    "    sim_doc_indices = np.argwhere(sim_matrix > threshold)\n",
    "    \n",
    "    sim_docs = defaultdict(list)\n",
    "    for pair in sim_doc_indices:\n",
    "        if doc_names:\n",
    "            sim_docs[doc_names[pair[0]]].append(doc_names[pair[1]])\n",
    "        else:\n",
    "            sim_docs[pair[0]].append(pair[1])\n",
    "\n",
    "    return sim_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_docs_by_shingles(dataset, n=7):\n",
    "    print(\"Generating Shingles for documents..\")\n",
    "    t0 = time.time()\n",
    "        \n",
    "    s = Shingling(n)\n",
    "    s.generate_shingles_for_docs(dataset)\n",
    "    sim_matrix = generate_sim_matrix(s.docs_shingles, s.doc_names)\n",
    "    sim_docs = retrieve_documents(sim_matrix, 0.3, shing.doc_names)\n",
    "    \n",
    "    print(\"Similar documents are: \\n\")\n",
    "    print(sim_docs)\n",
    "    print ('\\n Retrieving similar documents for ' + str(len(dataset)) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    "    return sim_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Shingles for documents..\n",
      "Shingling 10 articles...\n",
      "\n",
      "Shingling took 8.81 sec.\n",
      "Calculating the Similarity for all documents\n",
      "\n",
      "Similarity for 10 docs took 4.92 sec.\n",
      "\n",
      "Similarity Matrix\n",
      " [[0.    0.228 0.178 0.259 0.241 0.314 0.252 0.256 0.256 0.156]\n",
      " [0.228 0.    0.236 0.231 0.248 0.221 0.245 0.234 0.24  0.222]\n",
      " [0.178 0.236 0.    0.19  0.223 0.167 0.209 0.195 0.207 0.241]\n",
      " [0.259 0.231 0.19  0.    0.238 0.26  0.251 0.246 0.255 0.173]\n",
      " [0.241 0.248 0.223 0.238 0.    0.233 0.246 0.244 0.243 0.204]\n",
      " [0.314 0.221 0.167 0.26  0.233 0.    0.255 0.253 0.25  0.145]\n",
      " [0.252 0.245 0.209 0.251 0.246 0.255 0.    0.256 0.249 0.192]\n",
      " [0.256 0.234 0.195 0.246 0.244 0.253 0.256 0.    0.242 0.184]\n",
      " [0.256 0.24  0.207 0.255 0.243 0.25  0.249 0.242 0.    0.192]\n",
      " [0.156 0.222 0.241 0.173 0.204 0.145 0.192 0.184 0.192 0.   ]]\n",
      "Similar documents are: \n",
      "\n",
      "defaultdict(<class 'list'>, {'TheArrowofGold': ['ChanceATaleInTwoParts'], 'ChanceATaleInTwoParts': ['TheArrowofGold']})\n",
      "\n",
      " Retrieving similar documents for 10 docs took 13.73 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'ChanceATaleInTwoParts': ['TheArrowofGold'],\n",
       "             'TheArrowofGold': ['ChanceATaleInTwoParts']})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_similar_docs_by_shingles(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_docs_by_minhash(dataset, n=7, sim_score=0.15, k=200):\n",
    "    print(\"Generating Minhash for documents..\")\n",
    "    t0 = time.time()\n",
    "        \n",
    "    s = Shingling(n)\n",
    "    s.generate_shingles_for_docs(dataset)\n",
    "    m = MinHashing(k)\n",
    "    m.generate_doc_signatures(shing.docs_shingles)\n",
    "    \n",
    "    sim_matrix = generate_sim_matrix(m.docs_minhash_signatures, s.doc_names)\n",
    "    sim_docs = retrieve_documents(sim_matrix, sim_score, s.doc_names)\n",
    "    \n",
    "    print(\"Similar documents are: \\n\")\n",
    "    print(sim_docs)\n",
    "    print ('\\n Retrieving similar documents for ' + str(len(dataset)) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    "    return sim_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Minhash for documents..\n",
      "Shingling 10 articles...\n",
      "\n",
      "Shingling took 6.22 sec.\n",
      "Generating MinHash signatures for documents..\n",
      "\n",
      " Generating Signatures for 10 docs took 333.44 sec.\n",
      "Calculating the Similarity for all documents\n",
      "\n",
      "Similarity for 10 docs took 0.01 sec.\n",
      "\n",
      "Similarity Matrix\n",
      " [[0.    0.133 0.108 0.167 0.16  0.163 0.133 0.16  0.149 0.102]\n",
      " [0.133 0.    0.14  0.127 0.147 0.147 0.13  0.106 0.13  0.14 ]\n",
      " [0.108 0.14  0.    0.127 0.137 0.102 0.136 0.127 0.14  0.149]\n",
      " [0.167 0.127 0.127 0.    0.131 0.147 0.144 0.138 0.157 0.112]\n",
      " [0.16  0.147 0.137 0.131 0.    0.115 0.137 0.171 0.127 0.14 ]\n",
      " [0.163 0.147 0.102 0.147 0.115 0.    0.163 0.118 0.147 0.09 ]\n",
      " [0.133 0.13  0.136 0.144 0.137 0.163 0.    0.147 0.127 0.105]\n",
      " [0.16  0.106 0.127 0.138 0.171 0.118 0.147 0.    0.124 0.115]\n",
      " [0.149 0.13  0.14  0.157 0.127 0.147 0.127 0.124 0.    0.108]\n",
      " [0.102 0.14  0.149 0.112 0.14  0.09  0.105 0.115 0.108 0.   ]]\n",
      "Similar documents are: \n",
      "\n",
      "defaultdict(<class 'list'>, {})\n",
      "\n",
      " Retrieving similar documents for 10 docs took 339.68 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_similar_docs_by_minhash(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_docs_by_lsh(dataset, n=7, k=200, b=100, r=2, threshold=0.1):\n",
    "    print(\"Generating LSH for documents..\")\n",
    "    t0 = time.time()\n",
    "        \n",
    "    s = Shingling(n)\n",
    "    s.generate_shingles_for_docs(dataset)\n",
    "    m = MinHashing(k)\n",
    "    m.generate_doc_signatures(shing.docs_shingles)\n",
    "    lsh = LSH(b, r, threshold)\n",
    "    lsh.generate_candidate_pairs()\n",
    "\n",
    "    print(\"Similar documents are: \\n\")\n",
    "    print(lsh.candidate_pairs)\n",
    "    print ('\\n Retrieving similar documents for ' + str(len(dataset)) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    "    return lsh.candidate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LSH for documents..\n",
      "Shingling 10 articles...\n",
      "\n",
      "Shingling took 9.08 sec.\n",
      "Generating MinHash signatures for documents..\n"
     ]
    }
   ],
   "source": [
    "retrieve_similar_docs_by_lsh(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
