{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 -  Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "**Authors: Sherly Sherly and Anna Martignano**\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "The problem of discovering association rules between itemsets in a sales transaction database (a set of baskets) includes the following two sub-problems [R. Agrawal and R. Srikant, VLDB '94 (Links to an external site.)]:\n",
    "\n",
    "Finding frequent itemsets with support at least s;\n",
    "Generating association rules with confidence at least c from the itemsets found in the first step.\n",
    "Remind that an association rule is an implication X → Y, where X and Y are itemsets such that X∩Y=∅. Support of the rule X → Y is the number of transactions that contain X⋃Y. Confidence of the rule X → Y the fraction of transactions containing X⋃Y in all transactions that contain X.\n",
    "\n",
    "### 1.1 Task\n",
    "You are to solve the first sub-problem: to implement the Apriori algorithm for finding frequent itemsets with support at least $s$ in a dataset of sales transactions. Recall that support of an itemset is the number of transactions containing the itemset. To test and evaluate your implementation, write a program that uses your Apriori algorithm implementation to discover frequent itemsets with support at least s in a given dataset of sales transactions.\n",
    "\n",
    "The implementation can be done using any big data processing framework, such as Apache Spark, Apache Flink, or no framework, e.g., in Java, Python, etc.  \n",
    "\n",
    "### 1.2 Optional task for extra bonus\n",
    "Solve the second sub-problem, i.e., develop and implement an algorithm for generating association rules between frequent itemsets discovered by using the Apriori algorithm in a dataset of sales transactions. The rules must have support at least s and confidence at least c, where s and c are given as input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementations\n",
    "### 2.1 A-Priori Algorithm\n",
    "A two-pass approach called A-Priori limits the memory demand.\n",
    "\n",
    "Key idea: monotonicity of support\n",
    "- If a set of items appears at least s times, so does every subset, i.e., the support of a subset is at least as big as the support of its superset The downward closure property of frequent patterns\n",
    "- Any subset of a frequent itemset must be frequent. Contrapositive for pairs: if item i does not appear in s baskets, then no pair including i can appear in s baskets.\n",
    "\n",
    "Based on candidate generation-and-test approach A-priori pruning principle: If there is any itemset which is infrequent, its superset should not be generated/tested, because it’s also infrequent\n",
    "[Agrawal & Srikant,@VLDB’94, Mannila, et al. @ KDD’94]\n",
    "\n",
    "\n",
    "\n",
    "**Pass 1** : Read baskets and count in main memory the occurrences of\n",
    "each individual item\n",
    "- Requires O(n) memory, where n is #items. Items that appear $≥s$ times are the frequent items. Typical $s=1\\%$ as many singletons will be infrequent (s is the support threshold)\n",
    "\n",
    "\n",
    "**Pass 2**: Read baskets again and count only those pairs where both elements are frequent (discovered in Pass 1).\n",
    "- Requires memory proportional to square of frequent items only (for counts) – 2m instead 2n. Plus a list of the frequent items (so you know what must be counted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data():\n",
    "    data = {}\n",
    "    baskets = []\n",
    "    \n",
    "    # read baskets and count occurences\n",
    "    for line in open ('T10I4D100K.dat', 'r'):\n",
    "        basket = [int(item) for item in line.rstrip().split(\" \")] \n",
    "        for i in basket:\n",
    "            data[tuple([i])] = data.get(tuple([i]), 0) + 1\n",
    "        \n",
    "        baskets.append(basket)\n",
    "\n",
    "    # typically, s=1%\n",
    "    s = int (0.01 * len (baskets))\n",
    "\n",
    "    # keep only pairs that is above the support threshold\n",
    "    data = {k: v for k, v in data.items() if v >= s}\n",
    "\n",
    "    return data, baskets, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline of the A-Priory Algorithm**\n",
    "<img src=\"apriori_algo.png\">\n",
    "\n",
    "For each k, we construct two sets of k-tuples (sets of size k):\n",
    "- $C_{k}$ = candidate k-tuples = those that might be frequent sets (support $> s$) based on information from the pass for k–1\n",
    "- $L_{k}$ = the set of truly frequent k-tuples, i.e. filter only those k-tuples from $C_{k}$ that have support at least s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def get_frequent_Lset(previous_Lset, baskets, s, k):\n",
    "    new_Lset = {}\n",
    "\n",
    "    # Generate the set of items that is frequent based on last L\n",
    "    Lprev = set(flatten(previous_Lset.keys()))\n",
    "\n",
    "    \"\"\"\n",
    "    Second Pass:\n",
    "    (1) For each basket, look in the frequent-items\n",
    "        table to see which of its items are frequent.\n",
    "    (2) In a double loop, generate all pairs of frequent\n",
    "        items in that basket.\n",
    "    (3) For each such pair, add one to its count\n",
    "        in the data structure used to store counts.\n",
    "    \"\"\"\n",
    "    for basket in baskets:\n",
    "        # keep only frequent items in Lprev\n",
    "        valid_basket = list(Lprev.intersection(basket))\n",
    "        valid_basket.sort()\n",
    "\n",
    "        \"\"\"\n",
    "        For a candidate in Ck to be a frequent itemset,\n",
    "        all its subsets must be frequent, not only the\n",
    "        itemsets from Lk-1 and L1 that the candidate is\n",
    "        constructed from, i.e., each of its subsets should\n",
    "        be in the corresponding Lm, m = 1,…, k-1\n",
    "        \"\"\"\n",
    "        candidates = list(itertools.combinations(valid_basket, k))\n",
    "\n",
    "        for key in candidates:\n",
    "            prev_candidates = list(itertools.combinations(\n",
    "                list(key), k-1))\n",
    "\n",
    "            ## Check that all of the candidates exists\n",
    "            if len(set(prev_candidates).intersection(\n",
    "                    set(previous_Lset))) == len(prev_candidates):\n",
    "                new_Lset[key] = new_Lset.get(key, 0) + 1\n",
    "\n",
    "    # Filter the valid candidates\n",
    "    new_Lset = {k: v for k, v in new_Lset.items() if v >= s}\n",
    "\n",
    "    return new_Lset\n",
    "\n",
    "def apriori_algo():\n",
    "    # read L1, the baskets and compute the support threshold\n",
    "    L1, baskets, s = parse_data()\n",
    "\n",
    "    generated_set = [(1,)]\n",
    "    \n",
    "    # initialize the original set for L\n",
    "    Lset = L1\n",
    "    k = 2\n",
    "\n",
    "    # Generate pruned Lsets until it empties out\n",
    "    while (len(Lset) > 0):\n",
    "        generated_set.append(Lset)\n",
    "        Lset = get_frequent_Lset(Lset, baskets, s, k)\n",
    "        k += 1\n",
    "\n",
    "    return generated_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support threshold (1% of baskets-length): 1000\n"
     ]
    }
   ],
   "source": [
    "apriori = apriori_algo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Association Rules\n",
    "Generating association rules between frequent itemsets discovered by using the Apriori algorithm in a dataset of sales transactions. The rules must have support at least $s$ and confidence at least $c$, where $s$ and $c$ are given as input parameters.\n",
    "\n",
    "\n",
    "**Mining Association Rules**\n",
    "1. Find all frequent itemsets I with at least as a given support\n",
    "2. Rule generation\n",
    "    - For every subset A of I, generate a rule A → I \\ A\n",
    "         - Since I is frequent, then so is A\n",
    "         - Variant 1: Single pass to compute the rule confidence\n",
    "            - conf(A,B→C,D) = supp(A,B,C,D)/supp(A,B)\n",
    "         - Variant 2:\n",
    "            - Observation: If A,B,C→D is below confidence, so is A,B→C,D because of supp(A,B) ≥ supp(A,B,C)\n",
    "            - Can generate bigger rules from smaller ones\n",
    "\n",
    "Output the rules above the confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence(item_sets, s1, s2):\n",
    "    \"\"\"\n",
    "    conf(I -> j) = support(I union j) / support(I)\n",
    "    \"\"\"\n",
    "    union = list(set(s1).union(set(s2)))\n",
    "    # since the keys are sorted when we generate the itemsets\n",
    "    union.sort()\n",
    "\n",
    "    confidence = float(item_sets[len(union)][tuple(union)]) / item_sets[len(s1)][s1]\n",
    "\n",
    "    return confidence\n",
    "\n",
    "def generate_associations(item_sets, c):\n",
    "    \"\"\"\n",
    "    We are looking for rules I → j with reasonably high support\n",
    "    and confidence\n",
    "    \"\"\"\n",
    "    associations = {}\n",
    "    \n",
    "    # associations are computed only for item_sets that has at least 2 items\n",
    "    for k in range(2, len(item_sets)):\n",
    "        # for each num permutation in k\n",
    "        for count in range(1, k):\n",
    "            for items in item_sets[k]:\n",
    "                # I refers to I in the association I -> j\n",
    "                I = set(itertools.combinations(list(items), count))\n",
    "                for i in I:\n",
    "                    # a set that contains all items from both sets,\n",
    "                    # except items that are present in both sets\n",
    "                    # j refers to j in the association I -> j\n",
    "                    j = set(items).symmetric_difference(i)\n",
    "                    confidence = get_confidence(item_sets, i, j)\n",
    "                    if confidence > c:\n",
    "                        associations[tuple([tuple(i), tuple(j)])] = confidence\n",
    "\n",
    "    return associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.5\n",
    "associations = generate_associations(apriori, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(704,) → (39,) with confidence of 0.617056856187291\n",
      "(704,) → (825,) with confidence of 0.6142697881828316\n",
      "(227,) → (390,) with confidence of 0.577007700770077\n",
      "(704,) → (825, 39) with confidence of 0.5769230769230769\n",
      "(704, 825) → (39,) with confidence of 0.9392014519056261\n",
      "(39, 704) → (825,) with confidence of 0.9349593495934959\n",
      "(39, 825) → (704,) with confidence of 0.8719460825610783\n"
     ]
    }
   ],
   "source": [
    "for k, v in associations.items():\n",
    "    print(\"{} → {} with confidence of {}\".format(k[0], k[1], v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
